{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "openai.api_key = \"sk-xxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import html_parsing_ncbi, html_parsing_n2c2, get_classification_report, get_digit, get_macro_average_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 NCBI-Disease Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi_df = pd.read_csv('data/NER/NCBI-disease/test_200.csv')\n",
    "ncbi_example_df = pd.read_csv('data/NER/NCBI-disease/examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a helpful assistant to perform the following task.\n",
    "\"TASK: the task is to extract disease entities in a sentence.\"\n",
    "\"INPUT: the input is a sentence.\"\n",
    "\"OUTPUT: the output is an HTML that highlights all the disease entities in the sentence. The highlighting should only use HTML tags <span style=\\\"background-color: #FFFF00\\\"> and </span> and no other tags.\"\n",
    "\"\"\"\n",
    "def get_ner_ncbi_disease(sentence: str, gpt4: bool = False, shot: int = 0) -> str:\n",
    "    \"\"\"\n",
    "        Get NER prediction from GPT-3.5 or GPT-4 given a sentence in NCBI-disease dataset and some examples\n",
    "        Input:\n",
    "            sentence: a string of sentence\n",
    "            gpt4: whether to use GPT-4 or GPT-3.5\n",
    "            shot: number of examples to use\n",
    "        Output:\n",
    "            a HTML string that highlights all the disease entities in the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": system_message\n",
    "        }\n",
    "    ]\n",
    "    for i in range(shot):\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": ncbi_example_df.iloc[i]['text']\n",
    "            }\n",
    "        )\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": ncbi_example_df.iloc[i]['label_text']\n",
    "            }\n",
    "        )\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": sentence\n",
    "        }\n",
    "    )\n",
    "\n",
    "    gpt = \"gpt-4-1106-preview\" if gpt4 else \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "    retries = 10 # retry at most 10 times until it succeeds\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            time_start = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model = gpt,\n",
    "                messages = prompt,\n",
    "                temperature = 0.0, # deterministic\n",
    "                request_timeout = 60,\n",
    "                max_tokens = 4096,\n",
    "                n = 1,\n",
    "                seed = 42,\n",
    "                top_p = 0.95,\n",
    "            )\n",
    "            time_end = time.time()\n",
    "            return response['choices'][0]['message']['content'], time_end - time_start\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            print(f\"Retrying... {retries} retries left\")\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "    raise SystemExit(\"Max retries exceeded, exiting program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(ncbi_df), 1)):\n",
    "    ncbi_df.loc[i, 'html_gpt3.5_one_shot'], ncbi_df.loc[i, 'gpt3.5_one_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=False, shot=1)\n",
    "    ncbi_df.loc[i, 'html_gpt4_one_shot'], ncbi_df.loc[i, 'gpt4_one_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=True, shot=1)\n",
    "    ncbi_df.loc[i, 'html_gpt3.5_five_shot'], ncbi_df.loc[i, 'gpt3.5_five_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=False, shot=5)\n",
    "    ncbi_df.loc[i, 'html_gpt4_five_shot'], ncbi_df.loc[i, 'gpt4_five_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=True, shot=5)\n",
    "    ncbi_df.loc[i, 'html_gpt3.5_ten_shot'], ncbi_df.loc[i, 'gpt3.5_ten_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=False, shot=10)\n",
    "    ncbi_df.loc[i, 'html_gpt4_ten_shot'], ncbi_df.loc[i, 'gpt4_ten_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=True, shot=10)\n",
    "    ncbi_df.loc[i, 'html_gpt3.5_twenty_shot'], ncbi_df.loc[i, 'gpt3.5_twenty_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=False, shot=20)\n",
    "    ncbi_df.loc[i, 'html_gpt4_twenty_shot'], ncbi_df.loc[i, 'gpt4_twenty_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=True, shot=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 89th prediction because Gemini is not able to predict it due to safety filter\n",
    "ncbi_df.drop([89], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: you can just load the llm output from the csv file instead of running the above code\n",
    "# ncbi_df = pd.read_csv(\"data/NER/NCBI-disease/test_200_gpt_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi_df['gt_labels'], ncbi_df['gpt3.5_one_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt3.5_one_shot')\n",
    "_, ncbi_df['gpt4_one_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt4_one_shot')\n",
    "_, ncbi_df['gpt3.5_five_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt3.5_five_shot')\n",
    "_, ncbi_df['gpt4_five_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt4_five_shot')\n",
    "_, ncbi_df['gpt3.5_ten_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt3.5_ten_shot')\n",
    "_, ncbi_df['gpt4_ten_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt4_ten_shot')\n",
    "_, ncbi_df['gpt3.5_twenty_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt3.5_twenty_shot')\n",
    "_, ncbi_df['gpt4_twenty_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt4_twenty_shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score One Shot GPT 3.5 (Strict): 0.576271186440678\n",
      "F1 Score One Shot GPT 4 (Strict): 0.6251691474966171\n",
      "F1 Score Five Shot GPT 3.5 (Strict): 0.4444444444444445\n",
      "F1 Score Five Shot GPT 4 (Strict): 0.6582278481012659\n",
      "F1 Score Ten Shot GPT 3.5 (Strict): 0.40259740259740256\n",
      "F1 Score Ten Shot GPT 4 (Strict): 0.7035830618892509\n",
      "F1 Score Twenty Shot GPT 3.5 (Strict): 0.4507042253521127\n",
      "F1 Score Twenty Shot GPT 4 (Strict): 0.7229299363057323\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 Score One Shot GPT 3.5 (Strict): {get_classification_report(ncbi_df, 'gt_labels', 'gpt3.5_one_shot_labels', 'strict')['default']['f1-score']}\")\n",
    "print(f\"F1 Score One Shot GPT 4 (Strict): {get_classification_report(ncbi_df, 'gt_labels', 'gpt4_one_shot_labels', 'strict')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Five Shot GPT 3.5 (Strict): {get_classification_report(ncbi_df, 'gt_labels', 'gpt3.5_five_shot_labels', 'strict')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Five Shot GPT 4 (Strict): {get_classification_report(ncbi_df, 'gt_labels', 'gpt4_five_shot_labels', 'strict')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Ten Shot GPT 3.5 (Strict): {get_classification_report(ncbi_df, 'gt_labels', 'gpt3.5_ten_shot_labels', 'strict')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Ten Shot GPT 4 (Strict): {get_classification_report(ncbi_df, 'gt_labels', 'gpt4_ten_shot_labels', 'strict')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Twenty Shot GPT 3.5 (Strict): {get_classification_report(ncbi_df, 'gt_labels', 'gpt3.5_twenty_shot_labels', 'strict')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Twenty Shot GPT 4 (Strict): {get_classification_report(ncbi_df, 'gt_labels', 'gpt4_twenty_shot_labels', 'strict')['default']['f1-score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score One Shot GPT 3.5 (Lenient): 0.7196870925684485\n",
      "F1 Score One Shot GPT 4 (Lenient): 0.814614343707713\n",
      "F1 Score Five Shot GPT 3.5 (Lenient): 0.5925925925925926\n",
      "F1 Score Five Shot GPT 4 (Lenient): 0.8164556962025317\n",
      "F1 Score Ten Shot GPT 3.5 (Lenient): 0.5584415584415584\n",
      "F1 Score Ten Shot GPT 4 (Lenient): 0.8208469055374592\n",
      "F1 Score Twenty Shot GPT 3.5 (Lenient): 0.5492957746478873\n",
      "F1 Score Twenty Shot GPT 4 (Lenient): 0.8343949044585988\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 Score One Shot GPT 3.5 (Lenient): {get_classification_report(ncbi_df, 'gt_labels', 'gpt3.5_one_shot_labels', 'lenient')['default']['f1-score']}\")\n",
    "print(f\"F1 Score One Shot GPT 4 (Lenient): {get_classification_report(ncbi_df, 'gt_labels', 'gpt4_one_shot_labels', 'lenient')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Five Shot GPT 3.5 (Lenient): {get_classification_report(ncbi_df, 'gt_labels', 'gpt3.5_five_shot_labels', 'lenient')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Five Shot GPT 4 (Lenient): {get_classification_report(ncbi_df, 'gt_labels', 'gpt4_five_shot_labels', 'lenient')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Ten Shot GPT 3.5 (Lenient): {get_classification_report(ncbi_df, 'gt_labels', 'gpt3.5_ten_shot_labels', 'lenient')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Ten Shot GPT 4 (Lenient): {get_classification_report(ncbi_df, 'gt_labels', 'gpt4_ten_shot_labels', 'lenient')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Twenty Shot GPT 3.5 (Lenient): {get_classification_report(ncbi_df, 'gt_labels', 'gpt3.5_twenty_shot_labels', 'lenient')['default']['f1-score']}\")\n",
    "print(f\"F1 Score Twenty Shot GPT 4 (Lenient): {get_classification_report(ncbi_df, 'gt_labels', 'gpt4_twenty_shot_labels', 'lenient')['default']['f1-score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-3.5 one-shot prediction time: 2.46 seconds\n",
      "Average GPT-4 one-shot prediction time: 4.80 seconds\n",
      "Average GPT-3.5 five-shot prediction time: 2.26 seconds\n",
      "Average GPT-4 five-shot prediction time: 4.15 seconds\n",
      "Average GPT-3.5 ten-shot prediction time: 2.49 seconds\n",
      "Average GPT-4 ten-shot prediction time: 4.08 seconds\n",
      "Average GPT-3.5 twenty-shot prediction time: 2.47 seconds\n",
      "Average GPT-4 twenty-shot prediction time: 4.29 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average GPT-3.5 one-shot prediction time: {ncbi_df['gpt3.5_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 one-shot prediction time: {ncbi_df['gpt4_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 five-shot prediction time: {ncbi_df['gpt3.5_five_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 five-shot prediction time: {ncbi_df['gpt4_five_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 ten-shot prediction time: {ncbi_df['gpt3.5_ten_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 ten-shot prediction time: {ncbi_df['gpt4_ten_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 twenty-shot prediction time: {ncbi_df['gpt3.5_twenty_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 twenty-shot prediction time: {ncbi_df['gpt4_twenty_shot_time'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the inference results\n",
    "ncbi_df.to_csv('data/NER/NCBI-disease/test_200_gpt_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 2018 n2c2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_df = pd.read_csv('data/NER/2018_n2c2/test_200.csv')\n",
    "n2c2_example_df = pd.read_csv('data/NER/2018_n2c2/examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a helpful assistant to perform the following task.\n",
    "\"TASK: the task is to extract disease entities in a sentence. The entity type includes Form, Route, Frequency, Dosage, Strength, Duration, Reason, Ade, Drug.\"\n",
    "\"INPUT: the input is a sentence.\"\n",
    "\"OUTPUT: the output is an HTML that highlights all the disease entities in the sentence in different colors: Form(#FF0000), Route(#FFA500), Frequency(#FFFF00), Dosage(#00FF00), Strength(#0000FF), Duration(#800080), Reason(#FFC0CB), Ade(#964B00), Drug(#808080) in hex code. The highlighting should only use HTML tags <span style=\\\"background-color: #XXXXXX\\\"> and </span> and no other tags.\n",
    "\"\"\"\n",
    "def get_ner_2018_n2c2(sentence: str, gpt4: bool = False, shot: int = 0) -> str:\n",
    "    \"\"\"\n",
    "        Get NER prediction from GPT-3.5 or GPT-4 given a sentence in 2018 n2c2 dataset.\n",
    "        Input:\n",
    "            sentence: a string of sentence\n",
    "            gpt4: whether to use GPT-4 or GPT-3.5\n",
    "            shot: number of examples to use\n",
    "        Output:\n",
    "            a HTML string that highlights all the disease entities in the sentence in different colors\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": system_message\n",
    "        }\n",
    "    ]\n",
    "    for i in range(shot):\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": n2c2_example_df.iloc[i]['text']\n",
    "            }\n",
    "        )\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": n2c2_example_df.iloc[i]['label_text']\n",
    "            }\n",
    "        )\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": sentence\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    gpt = \"gpt-4-1106-preview\" if gpt4 else \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "    retries = 10 # retry at most 10 times until it succeeds\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            time_start = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model = gpt,\n",
    "                messages = prompt,\n",
    "                temperature = 0.0, # deterministic\n",
    "                request_timeout = 60,\n",
    "                max_tokens = 4096,\n",
    "                n = 1,\n",
    "                seed = 42,\n",
    "                top_p = 0.95,\n",
    "            )\n",
    "            time_end = time.time()\n",
    "            return response['choices'][0]['message']['content'], time_end - time_start\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            print(f\"Retrying... {retries} retries left\")\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "    raise SystemExit(\"Max retries exceeded, exiting program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(n2c2_df), 1)):\n",
    "    n2c2_df.loc[i, 'html_gpt3.5_one_shot'], n2c2_df.loc[i, 'gpt3.5_one_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=1)\n",
    "    n2c2_df.loc[i, 'html_gpt4_one_shot'], n2c2_df.loc[i, 'gpt4_one_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=1)\n",
    "    n2c2_df.loc[i, 'html_gpt3.5_five_shot'], n2c2_df.loc[i, 'gpt3.5_five_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=5)\n",
    "    n2c2_df.loc[i, 'html_gpt4_five_shot'], n2c2_df.loc[i, 'gpt4_five_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=5)\n",
    "    n2c2_df.loc[i, 'html_gpt3.5_ten_shot'], n2c2_df.loc[i, 'gpt3.5_ten_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=10)\n",
    "    n2c2_df.loc[i, 'html_gpt4_ten_shot'], n2c2_df.loc[i, 'gpt4_ten_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=10)\n",
    "    n2c2_df.loc[i, 'html_gpt3.5_twenty_shot'], n2c2_df.loc[i, 'gpt3.5_twenty_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=20)\n",
    "    n2c2_df.loc[i, 'html_gpt4_twenty_shot'], n2c2_df.loc[i, 'gpt4_twenty_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: you can just load the llm output from the csv file instead of running the above code\n",
    "n2c2_df = pd.read_csv(\"data/NER/2018_n2c2/test_200_gpt_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_df['gt_labels'], n2c2_df['gpt3.5_one_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt3.5_one_shot')\n",
    "_, n2c2_df['gpt4_one_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt4_one_shot')\n",
    "_, n2c2_df['gpt3.5_five_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt3.5_five_shot')\n",
    "_, n2c2_df['gpt4_five_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt4_five_shot')\n",
    "_, n2c2_df['gpt3.5_ten_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt3.5_ten_shot')\n",
    "_, n2c2_df['gpt4_ten_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt4_ten_shot')\n",
    "_, n2c2_df['gpt3.5_twenty_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt3.5_twenty_shot')\n",
    "_, n2c2_df['gpt4_twenty_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt4_twenty_shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score One Shot GPT 3.5 (Strict): 0.29077647291119374\n",
      "F1 Score Five Shot GPT 3.5 (Strict): 0.4838384617950764\n",
      "F1 Score Ten Shot GPT 3.5 (Strict): 0.5162000201626022\n",
      "F1 Score Twenty Shot GPT 3.5 (Strict): 0.5650956959965368\n",
      "F1 Score One Shot GPT 3.5 (Lenient): 0.42701067618461136\n",
      "F1 Score Five Shot GPT 3.5 (Lenient): 0.6197381788075881\n",
      "F1 Score Ten Shot GPT 3.5 (Lenient): 0.6435182915671569\n",
      "F1 Score Twenty Shot GPT 3.5 (Lenient): 0.7052064310047981\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 Score One Shot GPT 3.5 (Strict): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt3.5_one_shot_labels', 'strict'))}\")\n",
    "print(f\"F1 Score Five Shot GPT 3.5 (Strict): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt3.5_five_shot_labels', 'strict'))}\")\n",
    "print(f\"F1 Score Ten Shot GPT 3.5 (Strict): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt3.5_ten_shot_labels', 'strict'))}\")\n",
    "print(f\"F1 Score Twenty Shot GPT 3.5 (Strict): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt3.5_twenty_shot_labels', 'strict'))}\")\n",
    "print(f\"F1 Score One Shot GPT 3.5 (Lenient): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt3.5_one_shot_labels', 'lenient'))}\")\n",
    "print(f\"F1 Score Five Shot GPT 3.5 (Lenient): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt3.5_five_shot_labels', 'lenient'))}\")\n",
    "print(f\"F1 Score Ten Shot GPT 3.5 (Lenient): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt3.5_ten_shot_labels', 'lenient'))}\")\n",
    "print(f\"F1 Score Twenty Shot GPT 3.5 (Lenient): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt3.5_twenty_shot_labels', 'lenient'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score One Shot GPT 4 (Scrit): 0.4750154328666599\n",
      "F1 Score Five Shot GPT 4 (Scrit): 0.5816366473953379\n",
      "F1 Score Ten Shot GPT 4 (Scrit): 0.5898720406929092\n",
      "F1 Score Twenty Shot GPT 4 (Scrit): 0.616919788140017\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 Score One Shot GPT 4 (Scrit): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt4_one_shot_labels', 'strict'))}\")\n",
    "print(f\"F1 Score Five Shot GPT 4 (Scrit): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt4_five_shot_labels', 'strict'))}\")\n",
    "print(f\"F1 Score Ten Shot GPT 4 (Scrit): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt4_ten_shot_labels', 'strict'))}\")\n",
    "print(f\"F1 Score Twenty Shot GPT 4 (Scrit): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt4_twenty_shot_labels', 'strict'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score One Shot GPT 4 (Lenient): 0.6770438979451868\n",
      "F1 Score Five Shot GPT 4 (Lenient): 0.7500036015064712\n",
      "F1 Score Ten Shot GPT 4 Lenient): 0.7518670422875415\n",
      "F1 Score Twenty Shot GPT 4 (Lenient): 0.7497963260089207\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 Score One Shot GPT 4 (Lenient): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt4_one_shot_labels', 'lenient'))}\")\n",
    "print(f\"F1 Score Five Shot GPT 4 (Lenient): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt4_five_shot_labels', 'lenient'))}\")\n",
    "print(f\"F1 Score Ten Shot GPT 4 Lenient): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt4_ten_shot_labels', 'lenient'))}\")\n",
    "print(f\"F1 Score Twenty Shot GPT 4 (Lenient): {get_macro_average_f1(get_classification_report(n2c2_df, 'gt_labels', 'gpt4_twenty_shot_labels', 'lenient'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-3.5 one-shot prediction time: 4.74 seconds\n",
      "Average GPT-4 one-shot prediction time: 7.91 seconds\n",
      "Average GPT-3.5 five-shot prediction time: 4.30 seconds\n",
      "Average GPT-4 five-shot prediction time: 8.22 seconds\n",
      "Average GPT-3.5 ten-shot prediction time: 4.58 seconds\n",
      "Average GPT-4 ten-shot prediction time: 8.42 seconds\n",
      "Average GPT-3.5 twenty-shot prediction time: 4.70 seconds\n",
      "Average GPT-4 twenty-shot prediction time: 8.42 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average GPT-3.5 one-shot prediction time: {n2c2_df['gpt3.5_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 one-shot prediction time: {n2c2_df['gpt4_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 five-shot prediction time: {n2c2_df['gpt3.5_five_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 five-shot prediction time: {n2c2_df['gpt4_five_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 ten-shot prediction time: {n2c2_df['gpt3.5_ten_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 ten-shot prediction time: {n2c2_df['gpt4_ten_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 twenty-shot prediction time: {n2c2_df['gpt3.5_twenty_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 twenty-shot prediction time: {n2c2_df['gpt4_twenty_shot_time'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_df.to_csv('data/NER/2018_n2c2/test_200_gpt_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RE (Relation Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 2018 n2c2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Infernece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_df = pd.read_csv('data/RE/2018_n2c2/test_200.csv')\n",
    "n2c2_example_df = pd.read_csv('data/RE/2018_n2c2/examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a helpful assistant to perform the following task.\n",
    "\"TASK: the task is to classify relations for a sentence.\"\n",
    "\"INPUT: the input is a sentence where the entities are labeled within [E${X}] and [E${X}/] in a sentence, where X is an integer representing an unique entity.\"\n",
    "\"OUTPUT: your task is to select one out of the nine types of relations ('STRENGTH-DRUG', 'ROUTE-DRUG', 'FREQUENCY-DRUG', 'FORM-DRUG', 'DOSAGE-DRUG', 'REASON-DRUG', 'DURATION-DRUG', 'ADE-DRUG', and 'No relation').\"\n",
    "\"\"\"\n",
    "def get_re_2018_n2c2(sentence: str, gpt4: bool = False, shot: int = 0) -> str:\n",
    "    \"\"\"\n",
    "        Get RE prediction from GPT-3.5 or GPT-4 given a sentence in 2018 n2c2 dataset.\n",
    "        Args:\n",
    "            sentence: a string of sentence\n",
    "            gpt4: whether to use GPT-4 or GPT-3.5\n",
    "            shot: number of examples to use\n",
    "        Output:\n",
    "            a string of predicted relation\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": system_message\n",
    "        }\n",
    "    ]\n",
    "    for i in range(shot):\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": n2c2_example_df.iloc[i]['text']\n",
    "            }\n",
    "        )\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": n2c2_example_df.iloc[i]['labels']\n",
    "            }\n",
    "        )\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": sentence\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    gpt = \"gpt-4-1106-preview\" if gpt4 else \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "    retries = 10 # retry at most 10 times until it succeeds\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            time_start = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model = gpt,\n",
    "                messages = prompt,\n",
    "                temperature = 0.0, # deterministic\n",
    "                request_timeout = 60,\n",
    "                max_tokens = 4096,\n",
    "                n = 1,\n",
    "                seed = 42,\n",
    "                top_p = 0.95,\n",
    "            )\n",
    "            time_end = time.time()\n",
    "            return response['choices'][0]['message']['content'], time_end - time_start\n",
    "        except:\n",
    "            print(f\"Retrying... {retries} retries left\")\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "    raise SystemExit(\"Max retries exceeded, exiting program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(n2c2_df), 1)):\n",
    "    n2c2_df.loc[i, 'gpt3.5_one_shot'], n2c2_df.loc[i, 'gpt3.5_one_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=1)\n",
    "    n2c2_df.loc[i, 'gpt4_one_shot'], n2c2_df.loc[i, 'gpt4_one_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=1)\n",
    "    n2c2_df.loc[i, 'gpt3.5_five_shot'], n2c2_df.loc[i, 'gpt3.5_five_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=5)\n",
    "    n2c2_df.loc[i, 'gpt4_five_shot'], n2c2_df.loc[i, 'gpt4_five_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=5)\n",
    "    n2c2_df.loc[i, 'gpt3.5_ten_shot'], n2c2_df.loc[i, 'gpt3.5_ten_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=10)\n",
    "    n2c2_df.loc[i, 'gpt4_ten_shot'], n2c2_df.loc[i, 'gpt4_ten_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=10)\n",
    "    n2c2_df.loc[i, 'gpt3.5_twenty_shot'], n2c2_df.loc[i, 'gpt3.5_twenty_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=20)\n",
    "    n2c2_df.loc[i, 'gpt4_twenty_shot'], n2c2_df.loc[i, 'gpt4_twenty_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of ' ' if any\n",
    "n2c2_df['gpt3.5_one_shot'] = n2c2_df['gpt3.5_one_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt4_one_shot'] = n2c2_df['gpt4_one_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt3.5_five_shot'] = n2c2_df['gpt3.5_five_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt4_five_shot'] = n2c2_df['gpt4_five_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt3.5_ten_shot'] = n2c2_df['gpt3.5_ten_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt4_ten_shot'] = n2c2_df['gpt4_ten_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt3.5_twenty_shot'] = n2c2_df['gpt3.5_twenty_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt4_twenty_shot'] = n2c2_df['gpt4_twenty_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get digit label while considering failed LLM outputs as 'No relation'\n",
    "n2c2_df['labels'] = n2c2_df['labels'].apply(get_digit)\n",
    "n2c2_df['gpt3.5_one_shot_labels'] = n2c2_df['gpt3.5_one_shot'].apply(get_digit)\n",
    "n2c2_df['gpt4_one_shot_labels'] = n2c2_df['gpt4_one_shot'].apply(get_digit)\n",
    "n2c2_df['gpt3.5_five_shot_labels'] = n2c2_df['gpt3.5_five_shot'].apply(get_digit)\n",
    "n2c2_df['gpt4_five_shot_labels'] = n2c2_df['gpt4_five_shot'].apply(get_digit)\n",
    "n2c2_df['gpt3.5_ten_shot_labels'] = n2c2_df['gpt3.5_ten_shot'].apply(get_digit)\n",
    "n2c2_df['gpt4_ten_shot_labels'] = n2c2_df['gpt4_ten_shot'].apply(get_digit)\n",
    "n2c2_df['gpt3.5_twenty_shot_labels'] = n2c2_df['gpt3.5_twenty_shot'].apply(get_digit)\n",
    "n2c2_df['gpt4_twenty_shot_labels'] = n2c2_df['gpt4_twenty_shot'].apply(get_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: you can just load the llm output from the csv file instead of running the above code\n",
    "# n2c2_df = pd.read_csv(\"data/RE/2018_n2c2/test_200_gpt_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score GPT 3.5 One Shot: 0.1647954574783843\n",
      "F1 Score GPT 3.5 Five Shot: 0.16888155123449242\n",
      "F1 Score GPT 3.5 Ten Shot: 0.22564148687169894\n",
      "F1 Score GPT 3.5 Twenty Shot: 0.2905089036809467\n",
      "F1 Score GPT 4 One Shot: 0.6069838797527586\n",
      "F1 Score GPT 4 Five Shot: 0.7454425063120715\n",
      "F1 Score GPT 4 Ten Shot: 0.8522045130740783\n",
      "F1 Score GPT 4 Twenty Shot: 0.8821989810361903\n"
     ]
    }
   ],
   "source": [
    "y_true = n2c2_df['labels'].tolist()\n",
    "y_pred = n2c2_df['gpt3.5_one_shot_labels'].tolist()\n",
    "print(f\"F1 Score GPT 3.5 One Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = n2c2_df['gpt3.5_five_shot_labels'].tolist()\n",
    "print(f\"F1 Score GPT 3.5 Five Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = n2c2_df['gpt3.5_ten_shot_labels'].tolist()\n",
    "print(f\"F1 Score GPT 3.5 Ten Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = n2c2_df['gpt3.5_twenty_shot_labels'].tolist()\n",
    "print(f\"F1 Score GPT 3.5 Twenty Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = n2c2_df['gpt4_one_shot_labels'].tolist()\n",
    "print(f\"F1 Score GPT 4 One Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = n2c2_df['gpt4_five_shot_labels'].tolist()\n",
    "print(f\"F1 Score GPT 4 Five Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = n2c2_df['gpt4_ten_shot_labels'].tolist()\n",
    "print(f\"F1 Score GPT 4 Ten Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = n2c2_df['gpt4_twenty_shot_labels'].tolist()\n",
    "print(f\"F1 Score GPT 4 Twenty Shot: {f1_score(y_true, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-3.5 one-shot prediction time: 3.15 seconds\n",
      "Average GPT-4 one-shot prediction time: 1.28 seconds\n",
      "Average GPT-3.5 five-shot prediction time: 2.50 seconds\n",
      "Average GPT-4 five-shot prediction time: 0.81 seconds\n",
      "Average GPT-3.5 ten-shot prediction time: 3.24 seconds\n",
      "Average GPT-4 ten-shot prediction time: 0.75 seconds\n",
      "Average GPT-3.5 twenty-shot prediction time: 3.10 seconds\n",
      "Average GPT-4 twenty-shot prediction time: 0.90 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average GPT-3.5 one-shot prediction time: {n2c2_df['gpt3.5_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 one-shot prediction time: {n2c2_df['gpt4_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 five-shot prediction time: {n2c2_df['gpt3.5_five_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 five-shot prediction time: {n2c2_df['gpt4_five_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 ten-shot prediction time: {n2c2_df['gpt3.5_ten_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 ten-shot prediction time: {n2c2_df['gpt4_ten_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 twenty-shot prediction time: {n2c2_df['gpt3.5_twenty_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 twenty-shot prediction time: {n2c2_df['gpt4_twenty_shot_time'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the inference results\n",
    "n2c2_df.to_csv('data/RE/2018_n2c2/test_200_gpt_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gad_df = pd.read_csv('data/RE/GAD/test_200.csv')\n",
    "gad_example_df = pd.read_csv('data/RE/GAD/examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a helpful assistant to perform the following task.\n",
    "\"TASK: the task is to classify relations between a disease and a gene for a sentence.\"\n",
    "\"INPUT: the input is a sentence where the disease is labeled as @DISEASE$ and the gene is labeled as @GENE$ accordingly in a sentence. \"\n",
    "\"OUTPUT: your task is to select one out of the two types of relations (0 and 1) for the gene and disease without any explanation or other characters:\n",
    "0, no relations\n",
    "1, has relations\"\n",
    "\"\"\"\n",
    "def get_re_gad(sentence: str, gpt4: bool = False, shot: int = 0) -> str:\n",
    "    \"\"\"\n",
    "        Get RE prediction from GPT-3.5 or GPT-4 given a sentence in GAD dataset.\n",
    "        Args:\n",
    "            sentence: a string of sentence\n",
    "            gpt4: whether to use GPT-4 or GPT-3.5\n",
    "            shot: number of examples to use\n",
    "        Output:\n",
    "            a string of predicted relation\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": system_message,\n",
    "        }\n",
    "    ]\n",
    "    for i in range(shot):\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": gad_example_df.iloc[i]['text']\n",
    "            }\n",
    "        )\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": str(gad_example_df.iloc[i]['labels'])\n",
    "            }\n",
    "        )\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": sentence\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    gpt = \"gpt-4-1106-preview\" if gpt4 else \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "    retries = 10 # retry at most 10 times until it succeeds\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            time_start = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model = gpt,\n",
    "                messages = prompt,\n",
    "                temperature = 0.0, # deterministic\n",
    "                request_timeout = 60,\n",
    "                max_tokens = 4096,\n",
    "                n = 1,\n",
    "                seed = 42,\n",
    "                top_p = 0.95,\n",
    "            )\n",
    "            time_end = time.time()\n",
    "            return response['choices'][0]['message']['content'], time_end - time_start\n",
    "        except:\n",
    "            print(f\"Retrying... {retries} retries left\")\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "    raise SystemExit(\"Max retries exceeded, exiting program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(86, len(gad_df), 1)):\n",
    "    gad_df.loc[i, 'gpt3.5_one_shot'], gad_df.loc[i, 'gpt3.5_one_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=False, shot=1)\n",
    "    gad_df.loc[i, 'gpt4_one_shot'], gad_df.loc[i, 'gpt4_one_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=True, shot=1)\n",
    "    gad_df.loc[i, 'gpt3.5_five_shot'], gad_df.loc[i, 'gpt3.5_five_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=False, shot=5)\n",
    "    gad_df.loc[i, 'gpt4_five_shot'], gad_df.loc[i, 'gpt4_five_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=True, shot=5)\n",
    "    gad_df.loc[i, 'gpt3.5_ten_shot'], gad_df.loc[i, 'gpt3.5_ten_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=False, shot=10)\n",
    "    gad_df.loc[i, 'gpt4_ten_shot'], gad_df.loc[i, 'gpt4_ten_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=True, shot=10)\n",
    "    gad_df.loc[i, 'gpt3.5_twenty_shot'], gad_df.loc[i, 'gpt3.5_twenty_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=False, shot=20)\n",
    "    gad_df.loc[i, 'gpt4_twenty_shot'], gad_df.loc[i, 'gpt4_twenty_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=True, shot=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert some strings to int while considering failed LLM outputs as 'No relation (0)'\n",
    "gad_df['gpt3.5_one_shot_label'] = gad_df['gpt3.5_one_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt4_one_shot_label'] = gad_df['gpt4_one_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt3.5_five_shot_label'] = gad_df['gpt3.5_five_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt4_five_shot_label'] = gad_df['gpt4_five_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt3.5_ten_shot_label'] = gad_df['gpt3.5_ten_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt4_ten_shot_label'] = gad_df['gpt4_ten_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt3.5_twenty_shot_label'] = gad_df['gpt3.5_twenty_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt4_twenty_shot_label'] = gad_df['gpt4_twenty_shot'].apply(lambda x: int(x) if x.isdigit() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: you can just load the llm output from the csv file instead of running the above code\n",
    "# gad_df = pd.read_csv(\"data/RE/GAD/test_200_gpt_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score GPT 3.5 One Shot: 0.36755952380952384\n",
      "F1 Score GPT 3.5 Five Shot: 0.4241788958770091\n",
      "F1 Score GPT 3.5 Ten Shot: 0.4851199670476779\n",
      "F1 Score GPT 3.5 Twenty Shot: 0.46464305205436734\n",
      "F1 Score GPT 4 One Shot: 0.4206928513497856\n",
      "F1 Score GPT 4 Five Shot: 0.39929631854458075\n",
      "F1 Score GPT 4 Ten Shot: 0.5416887008637405\n",
      "F1 Score GPT 4 Twenty Shot: 0.5434253246753247\n"
     ]
    }
   ],
   "source": [
    "y_true = gad_df['labels'].tolist()\n",
    "y_pred = gad_df['gpt3.5_one_shot_label'].tolist()\n",
    "print(f\"F1 Score GPT 3.5 One Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = gad_df['gpt3.5_five_shot_label'].tolist()\n",
    "print(f\"F1 Score GPT 3.5 Five Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = gad_df['gpt3.5_ten_shot_label'].tolist()\n",
    "print(f\"F1 Score GPT 3.5 Ten Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = gad_df['gpt3.5_twenty_shot_label'].tolist()\n",
    "print(f\"F1 Score GPT 3.5 Twenty Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = gad_df['gpt4_one_shot_label'].tolist()\n",
    "print(f\"F1 Score GPT 4 One Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = gad_df['gpt4_five_shot_label'].tolist()\n",
    "print(f\"F1 Score GPT 4 Five Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = gad_df['gpt4_ten_shot_label'].tolist()\n",
    "print(f\"F1 Score GPT 4 Ten Shot: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "y_pred = gad_df['gpt4_twenty_shot_label'].tolist()\n",
    "print(f\"F1 Score GPT 4 Twenty Shot: {f1_score(y_true, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-3.5 one-shot prediction time: 2.61 seconds\n",
      "Average GPT-4 one-shot prediction time: 0.72 seconds\n",
      "Average GPT-3.5 five-shot prediction time: 2.32 seconds\n",
      "Average GPT-4 five-shot prediction time: 0.74 seconds\n",
      "Average GPT-3.5 ten-shot prediction time: 2.11 seconds\n",
      "Average GPT-4 ten-shot prediction time: 0.68 seconds\n",
      "Average GPT-3.5 twenty-shot prediction time: 2.42 seconds\n",
      "Average GPT-4 twenty-shot prediction time: 0.81 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average GPT-3.5 one-shot prediction time: {gad_df['gpt3.5_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 one-shot prediction time: {gad_df['gpt4_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 five-shot prediction time: {gad_df['gpt3.5_five_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 five-shot prediction time: {gad_df['gpt4_five_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 ten-shot prediction time: {gad_df['gpt3.5_ten_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 ten-shot prediction time: {gad_df['gpt4_ten_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 twenty-shot prediction time: {gad_df['gpt3.5_twenty_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 twenty-shot prediction time: {gad_df['gpt4_twenty_shot_time'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the inference results\n",
    "gad_df.to_csv('data/RE/GAD/test_200_gpt_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
